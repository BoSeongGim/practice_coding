{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261ceb4-e37d-42cf-a430-4d39a2c33a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of ANN (Artificial Neural Networks) using TensorFlow - using Keras API\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()   # MNIST data download.\n",
    "x_train, x_test = x_train.astype('float32'), x_test.astype('float32')        # Convert images to float32 data type.\n",
    "x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])      # Flatten the 28*28 image to 784 dimensions.\n",
    "x_train, x_test = x_train / 255., x_test / 255.                              # Normalize values between [0, 255] to values between [0, 1].\n",
    "y_train, y_test = tf.one_hot(y_train, depth=10), tf.one_hot(y_test, depth=10)# Apply one-hot encoding to label data.\n",
    "\n",
    "# Define settings for learning.\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30     # Number of studies\n",
    "batch_size = 256    # Number of batches\n",
    "display_step = 1    # Loss function output cycle\n",
    "input_size = 784    # 28 * 28\n",
    "hidden1_size = 256\n",
    "hidden2_size = 256\n",
    "output_size = 10\n",
    "\n",
    "# Use the tf.data API to mix data and retrieve it in batch format.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.shuffle(60000).batch(batch_size)\n",
    "\n",
    "def random_normal_intializer_with_stddev_1():\n",
    "  return tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None)\n",
    "\n",
    "# Define the ANN model using tf.keras.Model.\n",
    "class ANN(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(ANN, self).__init__()\n",
    "    self.hidden_layer_1 = tf.keras.layers.Dense(hidden1_size,\n",
    "                                                activation='relu',\n",
    "                                                kernel_initializer=random_normal_intializer_with_stddev_1(),\n",
    "                                                bias_initializer=random_normal_intializer_with_stddev_1())\n",
    "    self.hidden_layer_2 = tf.keras.layers.Dense(hidden2_size,\n",
    "                                                activation='relu',\n",
    "                                                kernel_initializer=random_normal_intializer_with_stddev_1(),\n",
    "                                                bias_initializer=random_normal_intializer_with_stddev_1())\n",
    "    self.output_layer = tf.keras.layers.Dense(output_size,\n",
    "                                              activation=None,\n",
    "                                              kernel_initializer=random_normal_intializer_with_stddev_1(),\n",
    "                                              bias_initializer=random_normal_intializer_with_stddev_1())\n",
    "\n",
    "  def call(self, x):\n",
    "    H1_output = self.hidden_layer_1(x)\n",
    "    H2_output = self.hidden_layer_2(H1_output)\n",
    "    logits = self.output_layer(H2_output)\n",
    "\n",
    "    return logits\n",
    "\n",
    "# Define the cross-entropy loss function.\n",
    "@tf.function\n",
    "def cross_entropy_loss(logits, y):\n",
    "  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "# Define the Adam optimizer for optimization.\n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Define a function for optimization.\n",
    "@tf.function\n",
    "def train_step(model, x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    y_pred = model(x)\n",
    "    loss = cross_entropy_loss(y_pred, y)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "# Define a function that outputs the modelâ€™s accuracy.\n",
    "@tf.function\n",
    "def compute_accuracy(y_pred, y):\n",
    "  correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "  return accuracy\n",
    "\n",
    "# Declare the ANN model.\n",
    "ANN_model = ANN()\n",
    "\n",
    "# Perform optimization a specified number of times.\n",
    "for epoch in range(num_epochs):\n",
    "  average_loss = 0.\n",
    "  total_batch = int(x_train.shape[0] / batch_size)\n",
    "  #Perform optimization on all batches.\n",
    "  for batch_x, batch_y in train_data:\n",
    "    # Run the optimizer to update parameters.\n",
    "    _, current_loss = train_step(ANN_model, batch_x, batch_y), cross_entropy_loss(ANN_model(batch_x), batch_y)\n",
    "    # Measure the average loss.\n",
    "    average_loss += current_loss / total_batch\n",
    "  # Learning results are output for each specified epoch.\n",
    "  if epoch % display_step == 0:\n",
    "    print(\"Epoch : %d, Loss : %f\" % ((epoch+1), average_loss))\n",
    "\n",
    "# Prints the accuracy of how accurate the model learned using test data is.\n",
    "print(\"Accuracy : %f\" % compute_accuracy(ANN_model(x_test), y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
